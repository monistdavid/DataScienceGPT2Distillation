{"cells":[{"cell_type":"markdown","metadata":{"id":"zi_QNUPv7dHo"},"source":["### First step, we load the data\n","\n","Functions for that purpose are contained in my_mnist_loader, and there is no need to go here into these details"]},{"cell_type":"code","source":["from google.colab import drive\n","drive.mount('/content/drive/')"],"metadata":{"colab":{"base_uri":"https://localhost:8080/"},"id":"Xn8KfwJ4CRE-","executionInfo":{"status":"ok","timestamp":1682031919064,"user_tz":240,"elapsed":1204,"user":{"displayName":"XinRu He","userId":"07942384921814832921"}},"outputId":"0a935d96-6698-427e-8a88-80e416b53564"},"execution_count":1,"outputs":[{"output_type":"stream","name":"stdout","text":["Drive already mounted at /content/drive/; to attempt to forcibly remount, call drive.mount(\"/content/drive/\", force_remount=True).\n"]}]},{"cell_type":"code","source":["import os\n","os.chdir(\"/content/drive/My Drive/Northeastern/data research project/Knowledge-Distillation-small\")"],"metadata":{"id":"KYW7TgZKCUeT","executionInfo":{"status":"ok","timestamp":1682031919065,"user_tz":240,"elapsed":4,"user":{"displayName":"XinRu He","userId":"07942384921814832921"}}},"execution_count":2,"outputs":[]},{"cell_type":"code","execution_count":3,"metadata":{"id":"mXxekT_87dHr","executionInfo":{"status":"ok","timestamp":1682031920113,"user_tz":240,"elapsed":1051,"user":{"displayName":"XinRu He","userId":"07942384921814832921"}}},"outputs":[],"source":["from my_mnist_loader import *"]},{"cell_type":"code","execution_count":4,"metadata":{"colab":{"base_uri":"https://localhost:8080/"},"id":"cfuR16hj7dHs","executionInfo":{"status":"ok","timestamp":1682031922695,"user_tz":240,"elapsed":2584,"user":{"displayName":"XinRu He","userId":"07942384921814832921"}},"outputId":"0cd9baae-c136-4196-9963-90003b8baed4"},"outputs":[{"output_type":"stream","name":"stdout","text":["Train data shape:  (50000, 784)\n","Train labels shape:  (50000,)\n","Validation data shape:  (10000, 784)\n","Validation labels shape:  (10000,)\n","Test data shape:  (10000, 784)\n","Test labels shape:  (10000,)\n"]}],"source":["X_train, y_train, X_val, y_val, X_test, y_test = my_load_data_wrapper()\n","print('Train data shape: ', X_train.shape)\n","print('Train labels shape: ', y_train.shape)\n","print('Validation data shape: ', X_val.shape)\n","print('Validation labels shape: ', y_val.shape)\n","print('Test data shape: ', X_test.shape)\n","print('Test labels shape: ', y_test.shape)"]},{"cell_type":"code","execution_count":5,"metadata":{"id":"K9ZzuG8K7dHt","executionInfo":{"status":"ok","timestamp":1682031922695,"user_tz":240,"elapsed":4,"user":{"displayName":"XinRu He","userId":"07942384921814832921"}}},"outputs":[],"source":["import matplotlib.pyplot as plt\n","%matplotlib inline"]},{"cell_type":"markdown","metadata":{"id":"aQ4y4Y5o7dHt"},"source":["Now we define a DataSet and DataLoaders for each of the original datasets\n","\n","DataSet and DataLoaders are PyTorch defined classes that make really easy to process data into our training loop"]},{"cell_type":"code","execution_count":6,"metadata":{"id":"WsRn2ze67dHu","executionInfo":{"status":"ok","timestamp":1682031928433,"user_tz":240,"elapsed":5741,"user":{"displayName":"XinRu He","userId":"07942384921814832921"}}},"outputs":[],"source":["from torch.utils.data import Dataset, DataLoader\n","class CustomDataset(Dataset):\n","    def __init__(self, features, labels):\n","        self.features = features\n","        self.labels = labels\n","\n","    def __getitem__(self, index):\n","        f = torch.tensor(self.features[index])\n","        l = torch.tensor(self.labels[index])\n","        return (f.to(device), l.to(device))\n","\n","    def __len__(self):\n","        return len(self.labels)"]},{"cell_type":"code","execution_count":7,"metadata":{"id":"Ibnfbtr67dHu","executionInfo":{"status":"ok","timestamp":1682031928434,"user_tz":240,"elapsed":22,"user":{"displayName":"XinRu He","userId":"07942384921814832921"}}},"outputs":[],"source":["# Create data loader\n","batch_size = 100\n","train_data = CustomDataset(X_train, y_train)\n","train_loader = DataLoader(dataset=train_data, batch_size=batch_size, shuffle=True)\n","val_data = CustomDataset(X_val, y_val)\n","val_loader = DataLoader(dataset=val_data, batch_size=batch_size, shuffle=True)\n","test_data = CustomDataset(X_test, y_test)\n","test_loader = DataLoader(dataset=test_data, batch_size=batch_size, shuffle=True)"]},{"cell_type":"code","execution_count":8,"metadata":{"id":"LdqUObPe7dHu","executionInfo":{"status":"ok","timestamp":1682031928434,"user_tz":240,"elapsed":21,"user":{"displayName":"XinRu He","userId":"07942384921814832921"}}},"outputs":[],"source":["def evaluate(model, dataset, max_ex=0):\n","    acc = 0\n","    N = len(dataset) * batch_size\n","    for i, (features, labels) in enumerate(dataset):\n","        scores = model(features)\n","        pred = torch.argmax(scores, dim=1)\n","        acc += torch.sum(torch.eq(pred, labels)).item()\n","        if max_ex != 0 and i >= max_ex:\n","            break\n","    # print(i)\n","    return (acc * 100 / ((i+1) * batch_size) )"]},{"cell_type":"markdown","metadata":{"id":"tAl74ihN7dHv"},"source":["### Define and train a big, teacher neural net\n","\n","First we import the PyTorch libraries, then create the neural network.\n","\n","However, instead of training now, you have the option of just loading the network I previously trained"]},{"cell_type":"code","execution_count":9,"metadata":{"id":"MvP5ZwJr7dHv","executionInfo":{"status":"ok","timestamp":1682031929566,"user_tz":240,"elapsed":1152,"user":{"displayName":"XinRu He","userId":"07942384921814832921"}}},"outputs":[],"source":["import torch\n","import torch.nn as nn\n","from torch.optim import Adam\n","from models import *\n","from plot_funcs import *"]},{"cell_type":"code","execution_count":10,"metadata":{"id":"Ei4zE8qv7dHv","executionInfo":{"status":"ok","timestamp":1682031929567,"user_tz":240,"elapsed":4,"user":{"displayName":"XinRu He","userId":"07942384921814832921"}}},"outputs":[],"source":["if torch.cuda.is_available():\n","    device = torch.device(\"cuda:0\")\n","else:\n","    device = torch.device(\"cpu\")"]},{"cell_type":"code","execution_count":11,"metadata":{"id":"1qrvEtDc7dHw","executionInfo":{"status":"ok","timestamp":1682031929567,"user_tz":240,"elapsed":3,"user":{"displayName":"XinRu He","userId":"07942384921814832921"}}},"outputs":[],"source":["class linear_net(nn.Module):\n","    def __init__(self, dropout=0.5):\n","        super(linear_net, self).__init__()\n","        self.linear_1 = nn.Linear(784, 1200)\n","        self.relu = nn.ReLU()\n","        self.dropout = nn.Dropout(p=dropout)\n","        self.linear_2 = nn.Linear(1200, 1200)\n","        self.dropout = nn.Dropout(p=dropout)\n","        self.linear_3 = nn.Linear(1200, 10)\n","\n","    def forward(self, input):\n","        scores = self.linear_1(input)\n","        scores = self.relu(scores)\n","        scores = self.linear_2(scores)\n","        scores = self.relu(scores)\n","        scores = self.dropout(scores)\n","        scores = self.linear_3(scores)\n","        return scores"]},{"cell_type":"code","execution_count":12,"metadata":{"id":"ie_lM4rR7dHw","executionInfo":{"status":"ok","timestamp":1682031934257,"user_tz":240,"elapsed":4693,"user":{"displayName":"XinRu He","userId":"07942384921814832921"}}},"outputs":[],"source":["# Create our teacher model\n","big_model = linear_net().to(device)"]},{"cell_type":"code","execution_count":13,"metadata":{"colab":{"base_uri":"https://localhost:8080/"},"id":"h4kqHI8Z7dHx","executionInfo":{"status":"ok","timestamp":1682031936655,"user_tz":240,"elapsed":2410,"user":{"displayName":"XinRu He","userId":"07942384921814832921"}},"outputId":"c599eeef-d812-498c-c40f-39b5008ea2ac"},"outputs":[{"output_type":"execute_result","data":{"text/plain":["linear_net(\n","  (linear_1): Linear(in_features=784, out_features=1200, bias=True)\n","  (relu): ReLU()\n","  (dropout): Dropout(p=0.5, inplace=False)\n","  (linear_2): Linear(in_features=1200, out_features=1200, bias=True)\n","  (linear_3): Linear(in_features=1200, out_features=10, bias=True)\n",")"]},"metadata":{},"execution_count":13}],"source":["# Load our pre-trained teacher model\n","load_path = \"teacher_linear_model/\"\n","checkpoint = torch.load(load_path + \"modelo\")\n","big_model.load_state_dict(checkpoint['model_state_dict'])\n","big_model.eval()"]},{"cell_type":"code","execution_count":null,"metadata":{"colab":{"base_uri":"https://localhost:8080/"},"id":"F-qEkEC07dHx","outputId":"1a34a8cc-eba2-4f8b-9c9e-9af10a234a5d"},"outputs":[{"output_type":"stream","name":"stderr","text":["100%|██████████| 500/500 [00:10<00:00, 46.50it/s]\n","100%|██████████| 500/500 [00:10<00:00, 45.93it/s]\n","100%|██████████| 500/500 [00:10<00:00, 45.53it/s]\n","100%|██████████| 500/500 [00:11<00:00, 43.87it/s]\n","100%|██████████| 500/500 [00:11<00:00, 44.60it/s] \n","100%|██████████| 500/500 [00:11<00:00, 45.05it/s]\n","100%|██████████| 500/500 [00:10<00:00, 48.37it/s]\n","100%|██████████| 500/500 [00:11<00:00, 43.72it/s]\n","100%|██████████| 500/500 [00:11<00:00, 45.24it/s]\n","100%|██████████| 500/500 [00:11<00:00, 45.38it/s]\n","100%|██████████| 500/500 [00:11<00:00, 44.31it/s]\n","100%|██████████| 500/500 [00:10<00:00, 48.44it/s]\n","100%|██████████| 500/500 [00:10<00:00, 46.14it/s]\n","100%|██████████| 500/500 [00:11<00:00, 44.91it/s]\n","100%|██████████| 500/500 [00:11<00:00, 43.73it/s]\n","100%|██████████| 500/500 [00:11<00:00, 44.63it/s]\n","100%|██████████| 500/500 [00:10<00:00, 46.41it/s]\n","100%|██████████| 500/500 [00:10<00:00, 47.65it/s]\n","100%|██████████| 500/500 [00:11<00:00, 45.02it/s]\n","100%|██████████| 500/500 [00:11<00:00, 44.81it/s]\n","100%|██████████| 500/500 [00:11<00:00, 44.04it/s] \n","100%|██████████| 500/500 [00:11<00:00, 45.03it/s]\n","100%|██████████| 500/500 [00:10<00:00, 48.02it/s]\n","100%|██████████| 500/500 [00:11<00:00, 45.02it/s]\n","100%|██████████| 500/500 [00:11<00:00, 44.84it/s]\n","100%|██████████| 500/500 [00:11<00:00, 44.62it/s]\n","100%|██████████| 500/500 [00:11<00:00, 43.51it/s]\n","100%|██████████| 500/500 [00:10<00:00, 46.08it/s]\n","100%|██████████| 500/500 [00:10<00:00, 46.48it/s]\n","100%|██████████| 500/500 [00:11<00:00, 44.09it/s]\n","100%|██████████| 500/500 [00:11<00:00, 44.29it/s]\n","100%|██████████| 500/500 [00:11<00:00, 43.82it/s]\n","100%|██████████| 500/500 [00:11<00:00, 44.27it/s]\n","100%|██████████| 500/500 [00:10<00:00, 46.39it/s]\n","100%|██████████| 500/500 [00:11<00:00, 43.91it/s]\n","100%|██████████| 500/500 [00:11<00:00, 44.33it/s]\n","100%|██████████| 500/500 [00:11<00:00, 43.80it/s]\n","100%|██████████| 500/500 [00:11<00:00, 43.76it/s]\n","100%|██████████| 500/500 [00:11<00:00, 43.89it/s]\n","100%|██████████| 500/500 [00:10<00:00, 46.21it/s]\n","100%|██████████| 500/500 [00:10<00:00, 45.55it/s]\n","100%|██████████| 500/500 [00:11<00:00, 44.56it/s]\n","100%|██████████| 500/500 [00:11<00:00, 44.33it/s]\n","100%|██████████| 500/500 [00:11<00:00, 44.29it/s]\n","100%|██████████| 500/500 [00:11<00:00, 42.46it/s]\n","100%|██████████| 500/500 [00:10<00:00, 47.02it/s]\n","100%|██████████| 500/500 [00:11<00:00, 45.45it/s]\n","100%|██████████| 500/500 [00:11<00:00, 44.30it/s]\n","100%|██████████| 500/500 [00:11<00:00, 44.04it/s]\n","100%|██████████| 500/500 [00:11<00:00, 44.05it/s]\n","100%|██████████| 500/500 [00:11<00:00, 43.77it/s]\n","100%|██████████| 500/500 [00:10<00:00, 46.70it/s]\n","100%|██████████| 500/500 [00:11<00:00, 44.73it/s]\n","100%|██████████| 500/500 [00:11<00:00, 44.04it/s]\n","100%|██████████| 500/500 [00:11<00:00, 43.88it/s]\n","100%|██████████| 500/500 [00:11<00:00, 44.01it/s]\n","100%|██████████| 500/500 [00:11<00:00, 43.81it/s]\n","100%|██████████| 500/500 [00:11<00:00, 43.14it/s]\n","100%|██████████| 500/500 [00:11<00:00, 44.74it/s]\n","100%|██████████| 500/500 [00:11<00:00, 43.77it/s]\n","100%|██████████| 500/500 [00:11<00:00, 43.04it/s]\n","100%|██████████| 500/500 [00:11<00:00, 43.54it/s]\n","100%|██████████| 500/500 [00:11<00:00, 43.54it/s]\n","100%|██████████| 500/500 [00:11<00:00, 44.58it/s]\n","100%|██████████| 500/500 [00:10<00:00, 45.91it/s]\n","100%|██████████| 500/500 [00:11<00:00, 43.07it/s]\n","100%|██████████| 500/500 [00:11<00:00, 43.57it/s]\n","100%|██████████| 500/500 [00:11<00:00, 43.39it/s]\n","100%|██████████| 500/500 [00:11<00:00, 43.76it/s]\n","100%|██████████| 500/500 [00:11<00:00, 43.89it/s]\n","100%|██████████| 500/500 [00:10<00:00, 46.39it/s]\n","100%|██████████| 500/500 [00:11<00:00, 43.52it/s]\n","100%|██████████| 500/500 [00:11<00:00, 43.10it/s]\n","100%|██████████| 500/500 [00:12<00:00, 40.70it/s]\n","100%|██████████| 500/500 [00:11<00:00, 43.53it/s]\n","100%|██████████| 500/500 [00:11<00:00, 43.52it/s]\n","100%|██████████| 500/500 [00:11<00:00, 43.78it/s]\n","100%|██████████| 500/500 [00:10<00:00, 45.48it/s]\n","100%|██████████| 500/500 [00:11<00:00, 43.56it/s]\n","100%|██████████| 500/500 [00:11<00:00, 42.45it/s]\n","100%|██████████| 500/500 [00:11<00:00, 43.09it/s]\n","100%|██████████| 500/500 [00:11<00:00, 42.73it/s]\n","100%|██████████| 500/500 [00:11<00:00, 43.21it/s]\n","100%|██████████| 500/500 [00:11<00:00, 44.60it/s]\n","100%|██████████| 500/500 [00:11<00:00, 45.34it/s]\n","100%|██████████| 500/500 [00:11<00:00, 43.33it/s]\n","100%|██████████| 500/500 [00:11<00:00, 42.86it/s]\n","100%|██████████| 500/500 [00:11<00:00, 42.62it/s]\n","100%|██████████| 500/500 [00:11<00:00, 43.22it/s]\n","100%|██████████| 500/500 [00:11<00:00, 43.01it/s]\n","100%|██████████| 500/500 [00:11<00:00, 45.29it/s]\n","100%|██████████| 500/500 [00:11<00:00, 44.06it/s]\n","100%|██████████| 500/500 [00:12<00:00, 38.89it/s]\n","100%|██████████| 500/500 [00:11<00:00, 42.74it/s]\n","100%|██████████| 500/500 [00:11<00:00, 43.24it/s]\n"," 38%|███▊      | 190/500 [00:04<00:03, 92.57it/s]"]}],"source":["# This cell is here if you want to train the teacher model yourself\n","# if you don't want to lose time DO NOT RUN\n","big_model = linear_net().to(device)\n","from tqdm import tqdm\n","%matplotlib inline\n","epochs = 1000\n","lr = 0.001\n","it = 0\n","output_dir = \"new_teacher_linear_model/\"\n","title = 'teacher_model'\n","it_per_epoch = 1\n","\n","train_loss = []\n","train_acc = []\n","val_acc = []\n","\n","# Loss function\n","loss_fn = nn.CrossEntropyLoss()\n","# Create optimizer\n","optimizer = Adam(big_model.parameters(), lr=lr)\n","epoch = 1000\n","for epoch in range(epochs):\n","    for features, labels in tqdm(train_loader):\n","        scores = big_model(features)\n","        loss = loss_fn(scores, labels)\n","        optimizer.zero_grad()\n","        loss.backward()\n","        optimizer.step()\n","        train_loss.append(loss.item())\n","        if it % 100 == 0:\n","            train_acc.append(evaluate(big_model, train_loader, max_ex=100))\n","            val_acc.append(evaluate(big_model, val_loader))\n","            plot_loss(train_loss, it, it_per_epoch, base_name=output_dir + \"loss_\"+title, title=title)\n","            plot_acc(train_acc, val_acc, it, it_per_epoch, base_name=output_dir + \"acc_\"+title, title=title)\n","        it += 1\n","# #perform last book keeping\n","# train_acc.append(evaluate(big_model, train_loader, max_ex=100))\n","# val_acc.append(evaluate(big_model, val_loader))\n","# plot_loss(train_loss, it, it_per_epoch, base_name=output_dir + \"loss_\"+title, title=title)\n","# plot_acc(train_acc, val_acc, it, it_per_epoch, base_name=output_dir + \"acc_\"+title, title=title)\n","\n","train_acc.append(evaluate(big_model, train_loader, max_ex=100))\n","val_acc.append(evaluate(big_model, val_loader))\n","plot_acc(train_acc, val_acc, it, it_per_epoch)"]},{"cell_type":"code","execution_count":null,"metadata":{"id":"twAYkvKS7dHx","executionInfo":{"status":"aborted","timestamp":1682031941305,"user_tz":240,"elapsed":11,"user":{"displayName":"XinRu He","userId":"07942384921814832921"}}},"outputs":[],"source":["train_acc = evaluate(big_model, train_loader)\n","print(\"\\nTrain accuracy: %.2f%%\" % train_acc)\n","val_acc = evaluate(big_model,val_loader)\n","print(\"Validation accuracy: %.2f%%\" % val_acc)\n","test_acc = evaluate(big_model, test_loader)\n","print(\"Test accuracy: %.2f%%\" % test_acc)"]},{"cell_type":"markdown","metadata":{"id":"vJvs18xV7dHy"},"source":["### Define and train a small, student neural net"]},{"cell_type":"code","execution_count":null,"metadata":{"id":"NwOik3zA7dHy","executionInfo":{"status":"aborted","timestamp":1682031941306,"user_tz":240,"elapsed":12,"user":{"displayName":"XinRu He","userId":"07942384921814832921"}}},"outputs":[],"source":["class small_linear_net(nn.Module):\n","    def __init__(self):\n","        super(small_linear_net, self).__init__()\n","        self.linear_1 = nn.Linear(784, 50)\n","        self.relu = nn.ReLU()\n","        self.linear_2 = nn.Linear(50, 10)\n","\n","    def forward(self, input):\n","        scores = self.linear_1(input)\n","        scores = self.relu(scores)\n","        scores = self.linear_2(scores)\n","        return scores"]},{"cell_type":"code","execution_count":null,"metadata":{"id":"_sI8sBD17dHy","executionInfo":{"status":"aborted","timestamp":1682031941306,"user_tz":240,"elapsed":11,"user":{"displayName":"XinRu He","userId":"07942384921814832921"}}},"outputs":[],"source":["# Create our student model\n","small_model = small_linear_net().to(device)"]},{"cell_type":"code","execution_count":null,"metadata":{"id":"tNPj_Fge7dHz","executionInfo":{"status":"aborted","timestamp":1682031941307,"user_tz":240,"elapsed":12,"user":{"displayName":"XinRu He","userId":"07942384921814832921"}}},"outputs":[],"source":["# Load our pre-trained student model\n","# This is just if you want to check the accuracy of this model\n","# trained with the original MNIST data\n","load_path = \"small_linear_model/\"\n","checkpoint = torch.load(load_path + \"modelo\")\n","small_model.load_state_dict(checkpoint['model_state_dict'])\n","small_model.eval()"]},{"cell_type":"code","execution_count":null,"metadata":{"id":"IsSmHwxa7dHz","executionInfo":{"status":"aborted","timestamp":1682031941308,"user_tz":240,"elapsed":13,"user":{"displayName":"XinRu He","userId":"07942384921814832921"}}},"outputs":[],"source":["train_acc = evaluate(small_model, train_loader)\n","print(\"\\nTrain accuracy: %.2f%%\" % train_acc)\n","val_acc = evaluate(small_model,val_loader)\n","print(\"Validation accuracy: %.2f%%\" % val_acc)\n","test_acc = evaluate(small_model, test_loader)\n","print(\"Test accuracy: %.2f%%\" % test_acc)"]},{"cell_type":"markdown","metadata":{"id":"k_rM2z-Q7dH0"},"source":["### Distillation training"]},{"cell_type":"code","execution_count":null,"metadata":{"id":"MCo_MeNY7dH0","executionInfo":{"status":"aborted","timestamp":1682031941308,"user_tz":240,"elapsed":13,"user":{"displayName":"XinRu He","userId":"07942384921814832921"}}},"outputs":[],"source":["# Set output directory and create if needed\n","import os\n","output_dir = \"small_linear_model_distill1/\"\n","if not os.path.exists(output_dir):\n","    os.makedirs(output_dir)"]},{"cell_type":"code","execution_count":null,"metadata":{"id":"SjsgOWkR7dH0","executionInfo":{"status":"aborted","timestamp":1682031941309,"user_tz":240,"elapsed":14,"user":{"displayName":"XinRu He","userId":"07942384921814832921"}}},"outputs":[],"source":["### Define our custom loss function\n","softmax_op = nn.Softmax(dim=1)\n","mseloss_fn = nn.MSELoss()\n","\n","def my_loss(scores, targets, T=5):\n","    soft_pred = softmax_op(scores / T)\n","    soft_targets = softmax_op(targets / T)\n","    loss = mseloss_fn(soft_pred, soft_targets)\n","    return loss"]},{"cell_type":"code","execution_count":null,"metadata":{"id":"FLTjoJEC7dH1","executionInfo":{"status":"aborted","timestamp":1682031941310,"user_tz":240,"elapsed":15,"user":{"displayName":"XinRu He","userId":"07942384921814832921"}}},"outputs":[],"source":["# Create a new student model to start training from zero\n","small_model = small_linear_net().to(device)\n","from tqdm import tqdm\n","%matplotlib inline\n","\n","# Hyperparameters\n","lr = 5e-3\n","epochs = 5\n","temp = 5\n","\n","# Create optimizer\n","optimizer = Adam(small_model.parameters(), lr=lr)\n","val_acc = []\n","train_acc = []\n","train_loss = [0]  # loss at iteration 0\n","for epoch in range(epochs):\n","    for features, labels in tqdm(train_loader):\n","        scores = small_model(features)\n","        targets = big_model(features)\n","        loss = my_loss(scores, targets, T = temp)\n","        optimizer.zero_grad()\n","        loss.backward()\n","        optimizer.step()\n","        # Book-keeping\n","        if it % 100 == 0:\n","            train_acc.append(evaluate(small_model, train_loader, max_ex=100))\n","            val_acc.append(evaluate(small_model, val_loader))\n","        it += 1\n","#perform last book-keeping\n","train_acc.append(evaluate(small_model, train_loader, max_ex=100))\n","val_acc.append(evaluate(small_model, val_loader))\n","plot_loss(train_loss, it, it_per_epoch)\n","plot_acc(train_acc, val_acc, it, it_per_epoch)"]},{"cell_type":"code","execution_count":null,"metadata":{"id":"-6wvAihA7dH1","executionInfo":{"status":"aborted","timestamp":1682031941310,"user_tz":240,"elapsed":15,"user":{"displayName":"XinRu He","userId":"07942384921814832921"}}},"outputs":[],"source":["train_acc = evaluate(small_model, train_loader)\n","print(\"\\nTrain accuracy: %.2f%%\" % train_acc)\n","val_acc = evaluate(small_model,val_loader)\n","print(\"Validation accuracy: %.2f%%\" % val_acc)\n","test_acc = evaluate(small_model, test_loader)\n","print(\"Test accuracy: %.2f%%\" % test_acc)"]}],"metadata":{"kernelspec":{"display_name":"Python 3","language":"python","name":"python3"},"language_info":{"codemirror_mode":{"name":"ipython","version":3},"file_extension":".py","mimetype":"text/x-python","name":"python","nbconvert_exporter":"python","pygments_lexer":"ipython3","version":"3.6.9"},"colab":{"provenance":[]},"accelerator":"GPU","gpuClass":"standard"},"nbformat":4,"nbformat_minor":0}